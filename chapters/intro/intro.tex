\chapter{Introduction}
\label{cha:intro}

 \section{Background}

 Metadata is "data that provides information about other data", but not directly about the content of the data, such as individual rows in a table or the pixels of an image. Categories of metadata commonly encountered include but are not limited to \cite{understandingMetadataRiley2017}:

\begin{itemize}
    \item Descriptive metadata - relates to finding and comprehending data. As an example, a book may consist of the title, author, subject, ISBN, etc;
    \item Administrative metadata - relates to how the data is intended to be used. Examples of this type are technical metadata (e.g.: format of a file, instructions for decoding), preservation or authenticity metadata (e.g.: versioning artefacts, digital signatures, check-sums) and rights of legal metadata (e.g.: licensing);
    \item Structural metadata - describing relationships of data to other resources;
    \item Statistical metadata - often used in statistics, describing the procurement, processing and aggregation done to create data sets \cite{roleOfMetadataInStatisticsdippo2000}. 
\end{itemize}

Big data refers to data sets that are too large and complex to be dealt with using traditional data processing methods and software while being at the centre of modern business and science. Until 2003, humans create roughly 5 exabytes ($10^{18}$ bytes) of stored data. In 2013, that amount was generated in two days \cite{bigDataAReviewsagiroglu2013}. Big data is characterized by its three V's \cite{understandingBigDataZikopoulos2011}:

\begin{itemize}
    \item Variety - the structure of data, which comes from many heterogeneous sources in different formats. Data may be structured, when it is warehoused, well organized, tagged and labelled, semi-structured or unstructured, where most data resides, random, difficult and costly to use effectively;
    \item Volume - the size of data, which is always increasing, is larger than the petabyte scale;
    \item Velocity - the speed at which data changes dictates the optimal way to use it. Data may be acquired in batches or arrive as a discrete stream of samples.
\end{itemize}

In the context of Big data, metadata is concerned with what is the data, where it comes from, how was it processed, what is the data type of each of its elements, what units is it measured in, what formulas were used to derive it, who owns it, governance processes, etc. Metadata is a core part of the effective use of big data.

Security is a major concern in the context of Big Data systems, relating to the protection of confidentiality, integrity and availability of data. Data centralised in one place opens itself up as a valuable attack target. Big Data privacy is also a relevant matter, concerned with protecting individuals' personal information (PII). These systems commonly store and aggregate large amounts of personal identifying information collected from a variety of sources, so mismanagement can greatly impact the users' privacy \cite{bigDataSecurityTankard20125}. There is an increasing number of regulations that organizations handling user data must follow, relating to how data is collected, where data is stored and how it is used. For example, the healthcare industry has been adopting the Big Data approach for storing patient records, which are inherently PII and are heavily regulated, thus privacy in those systems is a major point of contention \cite{bigDataSecurityHealthPatil2014}.

\section{Motivation}

Data is growing exponentially and soon all data may become Big Data. The Big Data approach has been widely adopted to handle large amounts of data in the computer science world for some time. Storing large quantities of data, while not a solved problem, has become more manageable with the decrease in the price of storage and processing units and the increase in the popularity of unstructured, file-based storage on distributed systems like AWS’s S3, Casandra, and HDFS. Doing large and complex computations on these data sets is greatly aided by the ability to detach computational power from storage, something DBMS are limited by, with data mesh tools such as Trino, Dremio, or Apache Drill.

These advancements are great but they don’t address an important point - storage and computing are not of much use if domain experts (i.e. not necessarily engineers) can't use them effectively. Historically, analysts would run queries to gather data, format it, plot graphs, write articles, etc. all manually with the goal of providing insights, but the industry is trending more towards automating some of these steps using giant SQL engines or data processing pipelines. This has been a great advancement but it brought along another problem - more derived data is getting generated, which is very hard to keep track of.

Gathering insights from data is finally a human process and our goal as engineers should be to assist this process by providing tooling, not to replace people with computers. In a data mesh, data lakes are connected to data lakes, data silos are connected to other data silos, comprising structured and unstructured, collected and derived, hand-picked and computer-generated data so it is becoming increasingly hard for domain experts to find and manage the data sets they are interested in or to discover new ones.

Metadata, data catalogues and data indexing systems are essential tools for data-driven organizations to maintain, govern and make use of data in the most efficient manner possible by allowing data analysts, data scientists, domain experts and administrators to understand the sprawling data meshes they are working with. Big Data systems have largely evolved without adjacent metadata management infrastructure, which can in part be seen as a good thing, empowering data interoperability and loosening constraints, but as these systems mature more advanced and complete solutions are needed to handle "Big Metadata" \cite{bigMetadataSmith2014}. As metadata and data discovery tools evolve, the need to consider the aspects of security, privacy and data access control is becoming more relevant.

\section{Technologies Used}

OpenMetadata is an open-source platform for metadata management that aims to help data-driven organizations govern their data and metadata in a flexible and extensible way, integrating with many industry-standard tools, processes and technologies. It addresses the challenges faced by organizations with large amounts of sprawling data with a platform that handles the management, definition, storage and procurement of the metadata for many different data resources, such as tables, columns, databases, files, dashboards, charts and also for the workflows and processes that resulted in those resources. Relating to the categories of metadata described above, OpenMetadata focuses on descriptive and structural but is flexible enough to accommodate any other type. The platform automates the process of metadata discovery and indexing from various sources, such as data lakes (e.g.: Trino), databases (e.g.: PostgreSQL), business intelligence tools (e.g.: PowerBI), messaging queues (e.g.: Apache Kafka), workflow orchestration tools (e.g.: Apache Airflow), etc. OpenMetadata is designed to be API-first \cite{apiFirstBealieu} so it fully-featured support programmatic access and also provides a user-friendly interface for data consumers, metadata maintainers, data analysts and scientists to explore data and collaborate.

Apache Ranger \cite{apacheRangerAccessControlModelGuptaMaanak2017,apacheRangerMultiLayerdGuptaMaanak2017} is an open-source security framework for enforcing fine-grained access control policies and auditing access to resources. It was developed by the Apache Foundation to integrate with systems in the Hadoop ecosystem and to tackle the security issues that commonly appear in big data applications. The framework offers a centralized point for security management across a large data-driven organization where administrators can create data access policies based on user or group permissions or data attributes (also commonly called tags) and comprehensive auditing functionality, empowering the monitoring of resource access and detection of malicious intent. Apache Ranger is widely used in healthcare 
to secure patient records \cite{rangerHealhcareRangarajan2018}, in telecommunications to protect customer information \cite{rangerTelecomAhmad2019} and in finance to comply with regulatory requirements. The framework easily integrates with other systems in the Hadoop ecosystem, such as HDFS, HBase, Solr and Kafka, but also with components outside of it, such as Trino, Presto and Elasticsearch.

\section{\label{sec:project_goal} Project Goal}

This project will be exploring methods of managing access control to metadata and metadata governance solutions. The work is done in the context of using OpenMetadata as the metadata store and Apache Ranger as the centralized security framework, to integrate these two systems.

We will be looking at OpenMetadata and its main functionalities, what systems for access control are already in place, what are their shortcomings and how these can be expanded. This project will also go over Apache Ranger's functionalities, how its plugins operate and a new plugin for OpenMetadata can be added. Ranger is notoriously badly documented, so this report also aims to serve as an introduction to the many parts of Apache Ranger and as a guide to integrating services with it.

\section{Current Solutions}

Open-source metadata catalogues and data discovery systems have been gaining traction in the previous years, as large companies such as LinkedIn, Lyft and Netflix have internal projects for the public. It is important to keep in mind that aimed that these projects were started to mould around each organization's specific workflows and pipelines, thus they are products of their environments and the set of features they offer reflects that. Examples of such systems include Amundsen Lyft, LinkedIn DatHub, and Netflix Metacat, which have similar features and functionalities compared to OpenMetadata but, just as similarly, have a lacking approach towards controlling access to metadata.

An outlier in the group of open-source solutions is Apache Atlas. Originally incubated by Hortonworks, it is now under the umbrella of the Apache Software Foundation which, some may argue, positions it at a higher standard of quality compared to the former, corporate-owned, projects. What makes Apache Atlas an outliner is not its ownership, but the project's attention given to data governance, access control for both data and metadata and privacy of users' data. Apache Atlas is part of the Hadoop ecosystems and natively integrates with Apache Ranger, offering specialized functionality which we will be covering in later parts of the report, Apache Hive, Apache Kafka and others, but is limited in interoperability with systems outside its ecosystems, an area where the former systems excel.

The realm of data cataloguing systems encompasses a significant portion of commercially available solutions, with Informatica being a notable exemplar. However, these solutions are proprietarily licensed, exorbitantly priced, and solely accessible to large-scale organizations. Consequently, assessing their features poses a challenge.
